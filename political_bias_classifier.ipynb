{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Bias in Tweets by Members of Congress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributors: Alex Shropshire, Mando Iwanaga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Use transfer learning (Pre-trained BERT model) to classify tweet text from Politicians as having a democratic bias, a republican bias, or neutrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Business Understanding  \n",
    "2.Understand the data  \n",
    "3.Prepare the data for analysis and modeling  \n",
    "4.Model  \n",
    "5.Evaluate Results  \n",
    "6.Deploy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Text Cleaning\n",
    "import re\n",
    "import string\n",
    "\n",
    "#Transfer Learning Model (BERT)\n",
    "from keras import Model\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from batch_generator.batch_generator import BatchGenerator\n",
    "from load_pretrained_bert import load_google_bert\n",
    "\n",
    "\n",
    "#Model Evaluation\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll begin by uploading our dataset retrieved from [figure-eight](https://www.figure-eight.com/data-for-everyone/), an open source data platform.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                              label   source  \\\n",
       "0    From: Trey Radel (Representative from Florida)  twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload dataset with only the necessary columns\n",
    "raw_df = pd.read_csv('Political-media-DFE.csv',encoding='latin')\n",
    "df = raw_df[['bias','message','embed','label','source','text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3689\n",
       "partisan    1311\n",
       "Name: bias, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset does not specify political affiliation, will need to add politician's affiliations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to clean text of puntuations\n",
    "def remove_punctuations(text):\n",
    "    '''Removes punctuation from strings'''\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Apply our remove_punctuations function\n",
    "df['text'] = df.loc[:,'text'].apply(remove_punctuations)\n",
    "df['label'] = df['label'].str.replace('From: ','')\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Create a new column with the tweet purpose and bias\n",
    "df['purpose_and_bias'] = df['message'] + '_' + df['bias']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan  \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan  \n",
       "2  please join me today in remembering our fallen...  support_neutral  \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral  \n",
       "4  amazon delivery drones show need to update law...  policy_partisan  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregorio</td>\n",
       "      <td>Sablan</td>\n",
       "      <td>Gregorio Sablan (Representative from NA)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>Robert Aderholt (Representative from Alabama)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lamar</td>\n",
       "      <td>Alexander</td>\n",
       "      <td>Lamar Alexander (Senator from Tennessee)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Justin</td>\n",
       "      <td>Amash</td>\n",
       "      <td>Justin Amash (Representative from Michigan)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>Amodei</td>\n",
       "      <td>Mark Amodei (Representative from Nevada)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      First       Last                                    congressman  \\\n",
       "0  Gregorio     Sablan       Gregorio Sablan (Representative from NA)   \n",
       "1    Robert   Aderholt  Robert Aderholt (Representative from Alabama)   \n",
       "2     Lamar  Alexander       Lamar Alexander (Senator from Tennessee)   \n",
       "3    Justin      Amash    Justin Amash (Representative from Michigan)   \n",
       "4      Mark     Amodei       Mark Amodei (Representative from Nevada)   \n",
       "\n",
       "  affiliation  \n",
       "0           d  \n",
       "1           r  \n",
       "2           r  \n",
       "3           r  \n",
       "4           r  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset of politicians and their affiliations\n",
    "congressmen_df = pd.read_csv('congressmen_2015.csv')\n",
    "congressmen_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the two datasets to include politicians affiliations and our target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partisan tweets will be labeled as the politicians affiliation in target column\n",
    "#neutral tweets will be labeled as neutral in target column\n",
    "df = df.merge(congressmen_df, how='left',left_on='label',right_on='congressman')\n",
    "df.loc[df.bias == 'partisan', 'target'] = df['affiliation']\n",
    "df.loc[df.bias == 'neutral', 'target'] = df['bias']\n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with target label as \"i\" \n",
    "df = df[df['target'] != 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Trey</td>\n",
       "      <td>Radel</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Schrader</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>d</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Crapo</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>r</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Udall</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \\\n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan   \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan   \n",
       "2  please join me today in remembering our fallen...  support_neutral   \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral   \n",
       "4  amazon delivery drones show need to update law...  policy_partisan   \n",
       "\n",
       "     First       Last                                 congressman affiliation  \\\n",
       "0     Trey      Radel    Trey Radel (Representative from Florida)           r   \n",
       "1    Mitch  McConnell     Mitch McConnell (Senator from Kentucky)           r   \n",
       "2     Kurt   Schrader  Kurt Schrader (Representative from Oregon)           d   \n",
       "3  Michael      Crapo          Michael Crapo (Senator from Idaho)           r   \n",
       "4     Mark      Udall          Mark Udall (Senator from Colorado)           d   \n",
       "\n",
       "    target  \n",
       "0        r  \n",
       "1        r  \n",
       "2  neutral  \n",
       "3  neutral  \n",
       "4        d  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    3631\n",
       "r           791\n",
       "d           490\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have 3 target classifications\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll create functions to clean our text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contraction(text):\n",
    "    \"\"\"Replace contractions from text\"\"\"\n",
    "    contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'can not'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_links(text, filler=' '):\n",
    "    \"\"\"Replace url links included in text\"\"\"\n",
    "    text = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*',\n",
    "                      filler, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\"Remove numbers from text\"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to incorporate three functions above in one\n",
    "def cleanText(text):\n",
    "    \"\"\"Incorporate three created functions above into one\"\"\"\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = replace_contraction(text)\n",
    "    text = replace_links(text, \"link\")\n",
    "    text = remove_numbers(text)\n",
    "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{]',\"\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label our target variables in single column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Trey</td>\n",
       "      <td>Radel</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Schrader</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Crapo</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>r</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Udall</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \\\n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan   \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan   \n",
       "2  please join me today in remembering our fallen...  support_neutral   \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral   \n",
       "4  amazon delivery drones show need to update law...  policy_partisan   \n",
       "\n",
       "     First       Last                                 congressman affiliation  \\\n",
       "0     Trey      Radel    Trey Radel (Representative from Florida)           r   \n",
       "1    Mitch  McConnell     Mitch McConnell (Senator from Kentucky)           r   \n",
       "2     Kurt   Schrader  Kurt Schrader (Representative from Oregon)           d   \n",
       "3  Michael      Crapo          Michael Crapo (Senator from Idaho)           r   \n",
       "4     Mark      Udall          Mark Udall (Senator from Colorado)           d   \n",
       "\n",
       "  target  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['target'] == 'neutral', 'target'] = 0\n",
    "df.loc[df['target'] == 'r', 'target'] = 1\n",
    "df.loc[df['target'] == 'd', 'target'] = 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip multi_cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload pretrained BERT model\n",
    "BERT_PRETRAINED_DIR = 'multi_cased_L-12_H-768_A-12/'\n",
    "SEQ_LEN = 70\n",
    "BATCH_SIZE = 12\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt nowthisnews rep trey radel r fl slams obamacare politics httpstcozvywmgyih\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define our X and Y variables and apply cleanText function\n",
    "X = df['text'].apply(cleanText).values\n",
    "Y = df['target'].values\n",
    "print(X[0])  \n",
    "print(Y[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split our data into training sets and test sets using default parameters\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684\n",
      "1228\n",
      "3684\n",
      "1228\n"
     ]
    }
   ],
   "source": [
    "#Sanity chack\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0622 01:58:08.570214 140133029328640 deprecation_wrapper.py:119] From /home/jupyter/political_bias_classifier_BERT/batch_generator/tokenization.py:74: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3684/3684 [00:02<00:00, 1573.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3684/3684 [00:00<00:00, 75790.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1228/1228 [00:00<00:00, 1725.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1228/1228 [00:00<00:00, 86477.59it/s]\n"
     ]
    }
   ],
   "source": [
    "#Define batch generators\n",
    "train_gen = BatchGenerator(X_train,\n",
    "                           vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                           seq_len=SEQ_LEN,\n",
    "                           labels=Y_train,\n",
    "                           do_lower_case=False,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "valid_gen = BatchGenerator(X_test,\n",
    "                           vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                           seq_len=SEQ_LEN,\n",
    "                           labels=Y_test,\n",
    "                           do_lower_case=False,\n",
    "                           batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 01:58:12.615245 140133029328640 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0622 01:58:12.634814 140133029328640 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0622 01:58:12.635972 140133029328640 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0622 01:58:12.677128 140133029328640 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0622 01:58:12.684548 140133029328640 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0622 01:58:12.807371 140133029328640 deprecation_wrapper.py:119] From /home/jupyter/political_bias_classifier_BERT/transformer/layers.py:75: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/embeddings/LayerNorm/beta  ->  layer_normalization_1/beta:0\n",
      "bert/embeddings/LayerNorm/gamma  ->  layer_normalization_1/gamma:0\n",
      "bert/embeddings/position_embeddings  ->  PositionEmbedding/embeddings:0\n",
      "bert/embeddings/token_type_embeddings  ->  SegmentEmbedding/embeddings:0\n",
      "bert/embeddings/word_embeddings  ->  TokenEmbedding/embeddings:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta  ->  layer_0/ln_1/beta:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma  ->  layer_0/ln_1/gamma:0\n",
      "bert/encoder/layer_0/attention/output/dense/bias  ->  layer_0/c_attn_proj/bias:0\n",
      "bert/encoder/layer_0/attention/output/dense/kernel  ->  layer_0/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_0/attention/self/key/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/key/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/attention/self/query/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/query/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/attention/self/value/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/value/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/intermediate/dense/bias  ->  layer_0/c_fc/bias:0\n",
      "bert/encoder/layer_0/intermediate/dense/kernel  ->  layer_0/c_fc/kernel:0\n",
      "bert/encoder/layer_0/output/LayerNorm/beta  ->  layer_0/ln_2/beta:0\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma  ->  layer_0/ln_2/gamma:0\n",
      "bert/encoder/layer_0/output/dense/bias  ->  layer_0/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_0/output/dense/kernel  ->  layer_0/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta  ->  layer_1/ln_1/beta:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma  ->  layer_1/ln_1/gamma:0\n",
      "bert/encoder/layer_1/attention/output/dense/bias  ->  layer_1/c_attn_proj/bias:0\n",
      "bert/encoder/layer_1/attention/output/dense/kernel  ->  layer_1/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_1/attention/self/key/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/key/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/attention/self/query/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/query/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/attention/self/value/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/value/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/intermediate/dense/bias  ->  layer_1/c_fc/bias:0\n",
      "bert/encoder/layer_1/intermediate/dense/kernel  ->  layer_1/c_fc/kernel:0\n",
      "bert/encoder/layer_1/output/LayerNorm/beta  ->  layer_1/ln_2/beta:0\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma  ->  layer_1/ln_2/gamma:0\n",
      "bert/encoder/layer_1/output/dense/bias  ->  layer_1/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_1/output/dense/kernel  ->  layer_1/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/beta  ->  layer_10/ln_1/beta:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/gamma  ->  layer_10/ln_1/gamma:0\n",
      "bert/encoder/layer_10/attention/output/dense/bias  ->  layer_10/c_attn_proj/bias:0\n",
      "bert/encoder/layer_10/attention/output/dense/kernel  ->  layer_10/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_10/attention/self/key/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/key/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/attention/self/query/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/query/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/attention/self/value/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/value/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/intermediate/dense/bias  ->  layer_10/c_fc/bias:0\n",
      "bert/encoder/layer_10/intermediate/dense/kernel  ->  layer_10/c_fc/kernel:0\n",
      "bert/encoder/layer_10/output/LayerNorm/beta  ->  layer_10/ln_2/beta:0\n",
      "bert/encoder/layer_10/output/LayerNorm/gamma  ->  layer_10/ln_2/gamma:0\n",
      "bert/encoder/layer_10/output/dense/bias  ->  layer_10/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_10/output/dense/kernel  ->  layer_10/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/beta  ->  layer_11/ln_1/beta:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/gamma  ->  layer_11/ln_1/gamma:0\n",
      "bert/encoder/layer_11/attention/output/dense/bias  ->  layer_11/c_attn_proj/bias:0\n",
      "bert/encoder/layer_11/attention/output/dense/kernel  ->  layer_11/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_11/attention/self/key/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/key/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/attention/self/query/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/query/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/attention/self/value/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/value/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/intermediate/dense/bias  ->  layer_11/c_fc/bias:0\n",
      "bert/encoder/layer_11/intermediate/dense/kernel  ->  layer_11/c_fc/kernel:0\n",
      "bert/encoder/layer_11/output/LayerNorm/beta  ->  layer_11/ln_2/beta:0\n",
      "bert/encoder/layer_11/output/LayerNorm/gamma  ->  layer_11/ln_2/gamma:0\n",
      "bert/encoder/layer_11/output/dense/bias  ->  layer_11/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_11/output/dense/kernel  ->  layer_11/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta  ->  layer_2/ln_1/beta:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma  ->  layer_2/ln_1/gamma:0\n",
      "bert/encoder/layer_2/attention/output/dense/bias  ->  layer_2/c_attn_proj/bias:0\n",
      "bert/encoder/layer_2/attention/output/dense/kernel  ->  layer_2/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_2/attention/self/key/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/key/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/attention/self/query/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/query/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/attention/self/value/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/value/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/intermediate/dense/bias  ->  layer_2/c_fc/bias:0\n",
      "bert/encoder/layer_2/intermediate/dense/kernel  ->  layer_2/c_fc/kernel:0\n",
      "bert/encoder/layer_2/output/LayerNorm/beta  ->  layer_2/ln_2/beta:0\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma  ->  layer_2/ln_2/gamma:0\n",
      "bert/encoder/layer_2/output/dense/bias  ->  layer_2/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_2/output/dense/kernel  ->  layer_2/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta  ->  layer_3/ln_1/beta:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma  ->  layer_3/ln_1/gamma:0\n",
      "bert/encoder/layer_3/attention/output/dense/bias  ->  layer_3/c_attn_proj/bias:0\n",
      "bert/encoder/layer_3/attention/output/dense/kernel  ->  layer_3/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_3/attention/self/key/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/key/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/attention/self/query/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/query/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/attention/self/value/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/value/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/intermediate/dense/bias  ->  layer_3/c_fc/bias:0\n",
      "bert/encoder/layer_3/intermediate/dense/kernel  ->  layer_3/c_fc/kernel:0\n",
      "bert/encoder/layer_3/output/LayerNorm/beta  ->  layer_3/ln_2/beta:0\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma  ->  layer_3/ln_2/gamma:0\n",
      "bert/encoder/layer_3/output/dense/bias  ->  layer_3/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_3/output/dense/kernel  ->  layer_3/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/beta  ->  layer_4/ln_1/beta:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma  ->  layer_4/ln_1/gamma:0\n",
      "bert/encoder/layer_4/attention/output/dense/bias  ->  layer_4/c_attn_proj/bias:0\n",
      "bert/encoder/layer_4/attention/output/dense/kernel  ->  layer_4/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_4/attention/self/key/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/key/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/attention/self/query/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/query/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/attention/self/value/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/value/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/intermediate/dense/bias  ->  layer_4/c_fc/bias:0\n",
      "bert/encoder/layer_4/intermediate/dense/kernel  ->  layer_4/c_fc/kernel:0\n",
      "bert/encoder/layer_4/output/LayerNorm/beta  ->  layer_4/ln_2/beta:0\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma  ->  layer_4/ln_2/gamma:0\n",
      "bert/encoder/layer_4/output/dense/bias  ->  layer_4/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_4/output/dense/kernel  ->  layer_4/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta  ->  layer_5/ln_1/beta:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma  ->  layer_5/ln_1/gamma:0\n",
      "bert/encoder/layer_5/attention/output/dense/bias  ->  layer_5/c_attn_proj/bias:0\n",
      "bert/encoder/layer_5/attention/output/dense/kernel  ->  layer_5/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_5/attention/self/key/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/key/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/attention/self/query/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/query/kernel  ->  layer_5/c_attn/kernel:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/encoder/layer_5/attention/self/value/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/value/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/intermediate/dense/bias  ->  layer_5/c_fc/bias:0\n",
      "bert/encoder/layer_5/intermediate/dense/kernel  ->  layer_5/c_fc/kernel:0\n",
      "bert/encoder/layer_5/output/LayerNorm/beta  ->  layer_5/ln_2/beta:0\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma  ->  layer_5/ln_2/gamma:0\n",
      "bert/encoder/layer_5/output/dense/bias  ->  layer_5/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_5/output/dense/kernel  ->  layer_5/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta  ->  layer_6/ln_1/beta:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma  ->  layer_6/ln_1/gamma:0\n",
      "bert/encoder/layer_6/attention/output/dense/bias  ->  layer_6/c_attn_proj/bias:0\n",
      "bert/encoder/layer_6/attention/output/dense/kernel  ->  layer_6/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_6/attention/self/key/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/key/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/attention/self/query/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/query/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/attention/self/value/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/value/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/intermediate/dense/bias  ->  layer_6/c_fc/bias:0\n",
      "bert/encoder/layer_6/intermediate/dense/kernel  ->  layer_6/c_fc/kernel:0\n",
      "bert/encoder/layer_6/output/LayerNorm/beta  ->  layer_6/ln_2/beta:0\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma  ->  layer_6/ln_2/gamma:0\n",
      "bert/encoder/layer_6/output/dense/bias  ->  layer_6/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_6/output/dense/kernel  ->  layer_6/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta  ->  layer_7/ln_1/beta:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma  ->  layer_7/ln_1/gamma:0\n",
      "bert/encoder/layer_7/attention/output/dense/bias  ->  layer_7/c_attn_proj/bias:0\n",
      "bert/encoder/layer_7/attention/output/dense/kernel  ->  layer_7/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_7/attention/self/key/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/key/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/attention/self/query/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/query/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/attention/self/value/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/value/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/intermediate/dense/bias  ->  layer_7/c_fc/bias:0\n",
      "bert/encoder/layer_7/intermediate/dense/kernel  ->  layer_7/c_fc/kernel:0\n",
      "bert/encoder/layer_7/output/LayerNorm/beta  ->  layer_7/ln_2/beta:0\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma  ->  layer_7/ln_2/gamma:0\n",
      "bert/encoder/layer_7/output/dense/bias  ->  layer_7/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_7/output/dense/kernel  ->  layer_7/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/beta  ->  layer_8/ln_1/beta:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/gamma  ->  layer_8/ln_1/gamma:0\n",
      "bert/encoder/layer_8/attention/output/dense/bias  ->  layer_8/c_attn_proj/bias:0\n",
      "bert/encoder/layer_8/attention/output/dense/kernel  ->  layer_8/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_8/attention/self/key/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/key/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/attention/self/query/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/query/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/attention/self/value/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/value/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/intermediate/dense/bias  ->  layer_8/c_fc/bias:0\n",
      "bert/encoder/layer_8/intermediate/dense/kernel  ->  layer_8/c_fc/kernel:0\n",
      "bert/encoder/layer_8/output/LayerNorm/beta  ->  layer_8/ln_2/beta:0\n",
      "bert/encoder/layer_8/output/LayerNorm/gamma  ->  layer_8/ln_2/gamma:0\n",
      "bert/encoder/layer_8/output/dense/bias  ->  layer_8/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_8/output/dense/kernel  ->  layer_8/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/beta  ->  layer_9/ln_1/beta:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/gamma  ->  layer_9/ln_1/gamma:0\n",
      "bert/encoder/layer_9/attention/output/dense/bias  ->  layer_9/c_attn_proj/bias:0\n",
      "bert/encoder/layer_9/attention/output/dense/kernel  ->  layer_9/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_9/attention/self/key/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/key/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/attention/self/query/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/query/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/attention/self/value/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/value/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/intermediate/dense/bias  ->  layer_9/c_fc/bias:0\n",
      "bert/encoder/layer_9/intermediate/dense/kernel  ->  layer_9/c_fc/kernel:0\n",
      "bert/encoder/layer_9/output/LayerNorm/beta  ->  layer_9/ln_2/beta:0\n",
      "bert/encoder/layer_9/output/LayerNorm/gamma  ->  layer_9/ln_2/gamma:0\n",
      "bert/encoder/layer_9/output/dense/bias  ->  layer_9/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_9/output/dense/kernel  ->  layer_9/c_ffn_proj/kernel:0\n",
      "not mapped:  bert/pooler/dense/bias\n",
      "not mapped:  bert/pooler/dense/kernel\n",
      "not mapped:  cls/predictions/output_bias\n",
      "not mapped:  cls/predictions/transform/LayerNorm/beta\n",
      "not mapped:  cls/predictions/transform/LayerNorm/gamma\n",
      "not mapped:  cls/predictions/transform/dense/bias\n",
      "not mapped:  cls/predictions/transform/dense/kernel\n",
      "not mapped:  cls/seq_relationship/output_bias\n",
      "not mapped:  cls/seq_relationship/output_weights\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 70, 768)      1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 70, 768)      393216      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 70, 768)      91812096    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 70, 768)      0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 70, 768)      1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 70, 768)      0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 70, 2304)     1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 70, 768)      0           layer_0/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 70, 768)      0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 70, 3072)     0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 70, 768)      0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 70, 768)      0           layer_1/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 70, 768)      0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 70, 3072)     0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 70, 768)      0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 70, 768)      0           layer_2/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 70, 768)      0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 70, 3072)     0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 70, 768)      0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 70, 768)      0           layer_3/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 70, 768)      0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 70, 3072)     0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 70, 768)      0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 70, 768)      0           layer_4/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 70, 768)      0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 70, 3072)     0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 70, 768)      0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 70, 768)      0           layer_5/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 70, 768)      0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 70, 3072)     0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 70, 768)      0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 70, 768)      0           layer_6/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 70, 768)      0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 70, 3072)     0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 70, 768)      0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 70, 768)      0           layer_7/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 70, 768)      0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 70, 3072)     0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 70, 768)      0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 70, 768)      0           layer_8/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 70, 768)      0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 70, 3072)     0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 70, 768)      0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 70, 768)      0           layer_9/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 70, 768)      0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 70, 3072)     0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 70, 768)      0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 70, 768)      0           layer_10/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 70, 768)      0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 70, 3072)     0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 70, 768)      0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 70, 768)      0           layer_11/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 70, 768)      0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 70, 3072)     0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 70, 768)      0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_2_add[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 177,262,848\n",
      "Trainable params: 177,262,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Load BERT pretrained model and print summary\n",
    "g_bert = load_google_bert(base_location=BERT_PRETRAINED_DIR, use_attn_mask=False, max_len=SEQ_LEN)\n",
    "g_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Layer 0 as containing the features relevant for classification; see BERT paper for further explanation on\n",
    "# this choice.\n",
    "classification_features = Lambda(lambda x: x[:, 0, :])(g_bert.output)\n",
    "out = Dense(3, activation='softmax')(classification_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 01:58:20.358971 140133029328640 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 70, 768)      1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 70, 768)      393216      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 70, 768)      91812096    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 70, 768)      0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 70, 768)      1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 70, 768)      0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 70, 2304)     1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 70, 768)      0           layer_0/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 70, 768)      0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 70, 3072)     0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 70, 768)      0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 70, 768)      0           layer_1/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 70, 768)      0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 70, 3072)     0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 70, 768)      0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 70, 768)      0           layer_2/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 70, 768)      0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 70, 3072)     0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 70, 768)      0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 70, 768)      0           layer_3/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 70, 768)      0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 70, 3072)     0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 70, 768)      0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 70, 768)      0           layer_4/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 70, 768)      0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 70, 3072)     0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 70, 768)      0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 70, 768)      0           layer_5/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 70, 768)      0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 70, 3072)     0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 70, 768)      0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 70, 768)      0           layer_6/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 70, 768)      0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 70, 3072)     0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 70, 768)      0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 70, 768)      0           layer_7/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 70, 768)      0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 70, 3072)     0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 70, 768)      0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 70, 768)      0           layer_8/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 70, 768)      0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 70, 3072)     0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 70, 768)      0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 70, 768)      0           layer_9/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 70, 768)      0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 70, 3072)     0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 70, 768)      0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 70, 768)      0           layer_10/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 70, 768)      0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 70, 3072)     0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 70, 768)      0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 70, 768)      0           layer_11/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 70, 768)      0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 70, 3072)     0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 70, 768)      0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            2307        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 177,265,155\n",
      "Trainable params: 177,265,155\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define model, compile, and define parameters\n",
    "model = Model(g_bert.inputs, out)\n",
    "model.compile(optimizer=Adam(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will now fit the BERT pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "307/307 [==============================] - 117s 382ms/step - loss: 0.5187 - acc: 0.7959 - val_loss: 0.6952 - val_acc: 0.7271\n"
     ]
    }
   ],
   "source": [
    "history_log = model.fit_generator(train_gen,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=valid_gen,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 8s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "#Generate class probability predictions \n",
    "Y_test_predictions = model.predict_generator(valid_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n",
      "1224\n"
     ]
    }
   ],
   "source": [
    "#Check if truth and pred length match, looks like they're not\n",
    "print(len(Y_test))\n",
    "print(len(Y_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make truth and pred length the same\n",
    "Y_test = Y_test[:len(Y_test_predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n",
      "1224\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "print(len(Y_test))\n",
    "print(len(Y_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9921198 , 0.00270515, 0.00517503],\n",
       "       [0.2083101 , 0.61396605, 0.17772388],\n",
       "       [0.07927147, 0.91105837, 0.00967021],\n",
       "       [0.9444568 , 0.0369842 , 0.01855901],\n",
       "       [0.41268933, 0.04572394, 0.54158676]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check some of the predictions\n",
    "Y_test_predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Probability Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = []\n",
    "for pred in Y_test_predictions:\n",
    "    if pred[2] > 0.4:\n",
    "        pred = 2\n",
    "        new_preds.append(pred)\n",
    "    elif (pred[2] <= 0.4) and (pred[1] > 0.5):\n",
    "        pred = 1\n",
    "        new_preds.append(pred)\n",
    "    else:\n",
    "        pred = 0\n",
    "        new_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_predictions = np.array(new_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 920]\n",
      " [  1 194]\n",
      " [  2 110]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_test_predictions, return_counts=True)\n",
    "\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7344771241830066\n",
      "Recall: 0.7344771241830066\n",
      "Precision: 0.7344771241830066\n",
      "F-1 Score: 0.7344771241830066\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test, Y_test_predictions)\n",
    "recall = recall_score(Y_test, Y_test_predictions, average='micro')\n",
    "precision = precision_score(Y_test, Y_test_predictions, average='micro')\n",
    "f1 = f1_score(Y_test, Y_test_predictions, average='micro')\n",
    "\n",
    "\n",
    "print (\"Accuracy: {}\".format(accuracy))\n",
    "print (\"Recall: {}\".format(recall))\n",
    "print (\"Precision: {}\".format(precision))\n",
    "print (\"F-1 Score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.795874049395614],\n",
       " 'loss': [0.5186771043009012],\n",
       " 'val_acc': [0.727124183493502],\n",
       " 'val_loss': [0.6952079083843559]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show loss metric\n",
    "#Since we used 1 epoch, it doesn't warrant a model loss plot\n",
    "history_log.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work and Potential Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test model on new tweets\n",
    "- Flask Deployment \n",
    "- Ranking: Most Neutral & Most Biased Congressmen\n",
    "- Strongest buzzwords for each affiliation\n",
    "- Comprehensive evaluation metrics with cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
