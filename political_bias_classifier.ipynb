{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train google BERT on Quora Insincere Questions Classification\n",
    "https://www.kaggle.com/c/quora-insincere-questions-classification.\n",
    "The model's task is to predict whether a question is sincere (label=0) or\n",
    "insincere (label=1)\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from batch_generator.batch_generator import BatchGenerator\n",
    "from load_pretrained_bert import load_google_bert\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                              label   source  \\\n",
       "0    From: Trey Radel (Representative from Florida)  twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('Political-media-DFE.csv',encoding='latin')\n",
    "df = raw_df[['bias','message','embed','label','source','text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3689\n",
       "partisan    1311\n",
       "Name: bias, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    '''Removes punctuation from strings'''\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df.loc[:,'text'].apply(remove_punctuations)\n",
    "df['label'] = df['label'].str.replace('From: ','')\n",
    "df['purpose_and_bias'] = df['message'] + '_' + df['bias']\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan  \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan  \n",
       "2  please join me today in remembering our fallen...  support_neutral  \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral  \n",
       "4  amazon delivery drones show need to update law...  policy_partisan  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregorio</td>\n",
       "      <td>Sablan</td>\n",
       "      <td>Gregorio Sablan (Representative from NA)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>Robert Aderholt (Representative from Alabama)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lamar</td>\n",
       "      <td>Alexander</td>\n",
       "      <td>Lamar Alexander (Senator from Tennessee)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Justin</td>\n",
       "      <td>Amash</td>\n",
       "      <td>Justin Amash (Representative from Michigan)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>Amodei</td>\n",
       "      <td>Mark Amodei (Representative from Nevada)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      First       Last                                    congressman  \\\n",
       "0  Gregorio     Sablan       Gregorio Sablan (Representative from NA)   \n",
       "1    Robert   Aderholt  Robert Aderholt (Representative from Alabama)   \n",
       "2     Lamar  Alexander       Lamar Alexander (Senator from Tennessee)   \n",
       "3    Justin      Amash    Justin Amash (Representative from Michigan)   \n",
       "4      Mark     Amodei       Mark Amodei (Representative from Nevada)   \n",
       "\n",
       "  affiliation  \n",
       "0           d  \n",
       "1           r  \n",
       "2           r  \n",
       "3           r  \n",
       "4           r  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congressmen_df = pd.read_csv('congressmen_2015.csv')\n",
    "congressmen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(congressmen_df, how='left',left_on='label',right_on='congressman')\n",
    "df.loc[df.bias == 'partisan', 'target'] = df['affiliation']\n",
    "df.loc[df.bias == 'neutral', 'target'] = df['bias']\n",
    "df.dropna(axis=0,inplace=True)\n",
    "df = df[df['target'] != 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Trey</td>\n",
       "      <td>Radel</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Schrader</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>d</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Crapo</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>r</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Udall</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \\\n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan   \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan   \n",
       "2  please join me today in remembering our fallen...  support_neutral   \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral   \n",
       "4  amazon delivery drones show need to update law...  policy_partisan   \n",
       "\n",
       "     First       Last                                 congressman affiliation  \\\n",
       "0     Trey      Radel    Trey Radel (Representative from Florida)           r   \n",
       "1    Mitch  McConnell     Mitch McConnell (Senator from Kentucky)           r   \n",
       "2     Kurt   Schrader  Kurt Schrader (Representative from Oregon)           d   \n",
       "3  Michael      Crapo          Michael Crapo (Senator from Idaho)           r   \n",
       "4     Mark      Udall          Mark Udall (Senator from Colorado)           d   \n",
       "\n",
       "    target  \n",
       "0        r  \n",
       "1        r  \n",
       "2  neutral  \n",
       "3  neutral  \n",
       "4        d  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    3631\n",
       "r           791\n",
       "d           490\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contraction(text):\n",
    "    contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'can not'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "def replace_links(text, filler=' '):\n",
    "        text = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*',\n",
    "                      filler, text).strip()\n",
    "        return text\n",
    "def remove_numbers(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = replace_contraction(text)\n",
    "    text = replace_links(text, \"link\")\n",
    "    text = remove_numbers(text)\n",
    "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{]',\"\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Trey</td>\n",
       "      <td>Radel</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Schrader</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Crapo</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>r</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Udall</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \\\n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan   \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan   \n",
       "2  please join me today in remembering our fallen...  support_neutral   \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral   \n",
       "4  amazon delivery drones show need to update law...  policy_partisan   \n",
       "\n",
       "     First       Last                                 congressman affiliation  \\\n",
       "0     Trey      Radel    Trey Radel (Representative from Florida)           r   \n",
       "1    Mitch  McConnell     Mitch McConnell (Senator from Kentucky)           r   \n",
       "2     Kurt   Schrader  Kurt Schrader (Representative from Oregon)           d   \n",
       "3  Michael      Crapo          Michael Crapo (Senator from Idaho)           r   \n",
       "4     Mark      Udall          Mark Udall (Senator from Colorado)           d   \n",
       "\n",
       "  target  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      2  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['target'] == 'neutral', 'target'] = 0\n",
    "df.loc[df['target'] == 'r', 'target'] = 1\n",
    "df.loc[df['target'] == 'd', 'target'] = 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = 'multi_cased_L-12_H-768_A-12/'\n",
    "SEQ_LEN = 70\n",
    "BATCH_SIZE = 12\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt nowthisnews rep trey radel r fl slams obamacare politics httpstcozvywmgyih\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X = df['text'].apply(cleanText).values\n",
    "Y = df['target'].values\n",
    "print(X[0])  # How did Quebec nationalists see their province as a nation in the 1960s?\n",
    "print(Y[0])  # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684\n",
      "1228\n",
      "3684\n",
      "1228\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3684/3684 [00:02<00:00, 1265.01it/s]\n",
      "100%|██████████| 3684/3684 [00:00<00:00, 53884.70it/s]\n",
      "100%|██████████| 1228/1228 [00:00<00:00, 1366.40it/s]\n",
      "100%|██████████| 1228/1228 [00:00<00:00, 40200.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_gen = BatchGenerator(X_train,\n",
    "                           vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                           seq_len=SEQ_LEN,\n",
    "                           labels=Y_train,\n",
    "                           do_lower_case=False,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "valid_gen = BatchGenerator(X_test,\n",
    "                           vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                           seq_len=SEQ_LEN,\n",
    "                           labels=Y_test,\n",
    "                           do_lower_case=False,\n",
    "                           batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "bert/embeddings/LayerNorm/beta  ->  layer_normalization_1/beta:0\n",
      "bert/embeddings/LayerNorm/gamma  ->  layer_normalization_1/gamma:0\n",
      "bert/embeddings/position_embeddings  ->  PositionEmbedding/embeddings:0\n",
      "bert/embeddings/token_type_embeddings  ->  SegmentEmbedding/embeddings:0\n",
      "bert/embeddings/word_embeddings  ->  TokenEmbedding/embeddings:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta  ->  layer_0/ln_1/beta:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma  ->  layer_0/ln_1/gamma:0\n",
      "bert/encoder/layer_0/attention/output/dense/bias  ->  layer_0/c_attn_proj/bias:0\n",
      "bert/encoder/layer_0/attention/output/dense/kernel  ->  layer_0/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_0/attention/self/key/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/key/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/attention/self/query/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/query/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/attention/self/value/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/value/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/intermediate/dense/bias  ->  layer_0/c_fc/bias:0\n",
      "bert/encoder/layer_0/intermediate/dense/kernel  ->  layer_0/c_fc/kernel:0\n",
      "bert/encoder/layer_0/output/LayerNorm/beta  ->  layer_0/ln_2/beta:0\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma  ->  layer_0/ln_2/gamma:0\n",
      "bert/encoder/layer_0/output/dense/bias  ->  layer_0/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_0/output/dense/kernel  ->  layer_0/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta  ->  layer_1/ln_1/beta:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma  ->  layer_1/ln_1/gamma:0\n",
      "bert/encoder/layer_1/attention/output/dense/bias  ->  layer_1/c_attn_proj/bias:0\n",
      "bert/encoder/layer_1/attention/output/dense/kernel  ->  layer_1/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_1/attention/self/key/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/key/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/attention/self/query/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/query/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/attention/self/value/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/value/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/intermediate/dense/bias  ->  layer_1/c_fc/bias:0\n",
      "bert/encoder/layer_1/intermediate/dense/kernel  ->  layer_1/c_fc/kernel:0\n",
      "bert/encoder/layer_1/output/LayerNorm/beta  ->  layer_1/ln_2/beta:0\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma  ->  layer_1/ln_2/gamma:0\n",
      "bert/encoder/layer_1/output/dense/bias  ->  layer_1/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_1/output/dense/kernel  ->  layer_1/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/beta  ->  layer_10/ln_1/beta:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/gamma  ->  layer_10/ln_1/gamma:0\n",
      "bert/encoder/layer_10/attention/output/dense/bias  ->  layer_10/c_attn_proj/bias:0\n",
      "bert/encoder/layer_10/attention/output/dense/kernel  ->  layer_10/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_10/attention/self/key/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/key/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/attention/self/query/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/query/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/attention/self/value/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/value/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/intermediate/dense/bias  ->  layer_10/c_fc/bias:0\n",
      "bert/encoder/layer_10/intermediate/dense/kernel  ->  layer_10/c_fc/kernel:0\n",
      "bert/encoder/layer_10/output/LayerNorm/beta  ->  layer_10/ln_2/beta:0\n",
      "bert/encoder/layer_10/output/LayerNorm/gamma  ->  layer_10/ln_2/gamma:0\n",
      "bert/encoder/layer_10/output/dense/bias  ->  layer_10/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_10/output/dense/kernel  ->  layer_10/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/beta  ->  layer_11/ln_1/beta:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/gamma  ->  layer_11/ln_1/gamma:0\n",
      "bert/encoder/layer_11/attention/output/dense/bias  ->  layer_11/c_attn_proj/bias:0\n",
      "bert/encoder/layer_11/attention/output/dense/kernel  ->  layer_11/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_11/attention/self/key/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/key/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/attention/self/query/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/query/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/attention/self/value/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/value/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/intermediate/dense/bias  ->  layer_11/c_fc/bias:0\n",
      "bert/encoder/layer_11/intermediate/dense/kernel  ->  layer_11/c_fc/kernel:0\n",
      "bert/encoder/layer_11/output/LayerNorm/beta  ->  layer_11/ln_2/beta:0\n",
      "bert/encoder/layer_11/output/LayerNorm/gamma  ->  layer_11/ln_2/gamma:0\n",
      "bert/encoder/layer_11/output/dense/bias  ->  layer_11/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_11/output/dense/kernel  ->  layer_11/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta  ->  layer_2/ln_1/beta:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma  ->  layer_2/ln_1/gamma:0\n",
      "bert/encoder/layer_2/attention/output/dense/bias  ->  layer_2/c_attn_proj/bias:0\n",
      "bert/encoder/layer_2/attention/output/dense/kernel  ->  layer_2/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_2/attention/self/key/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/key/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/attention/self/query/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/query/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/attention/self/value/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/value/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/intermediate/dense/bias  ->  layer_2/c_fc/bias:0\n",
      "bert/encoder/layer_2/intermediate/dense/kernel  ->  layer_2/c_fc/kernel:0\n",
      "bert/encoder/layer_2/output/LayerNorm/beta  ->  layer_2/ln_2/beta:0\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma  ->  layer_2/ln_2/gamma:0\n",
      "bert/encoder/layer_2/output/dense/bias  ->  layer_2/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_2/output/dense/kernel  ->  layer_2/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta  ->  layer_3/ln_1/beta:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma  ->  layer_3/ln_1/gamma:0\n",
      "bert/encoder/layer_3/attention/output/dense/bias  ->  layer_3/c_attn_proj/bias:0\n",
      "bert/encoder/layer_3/attention/output/dense/kernel  ->  layer_3/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_3/attention/self/key/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/key/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/attention/self/query/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/query/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/attention/self/value/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/value/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/intermediate/dense/bias  ->  layer_3/c_fc/bias:0\n",
      "bert/encoder/layer_3/intermediate/dense/kernel  ->  layer_3/c_fc/kernel:0\n",
      "bert/encoder/layer_3/output/LayerNorm/beta  ->  layer_3/ln_2/beta:0\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma  ->  layer_3/ln_2/gamma:0\n",
      "bert/encoder/layer_3/output/dense/bias  ->  layer_3/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_3/output/dense/kernel  ->  layer_3/c_ffn_proj/kernel:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/encoder/layer_4/attention/output/LayerNorm/beta  ->  layer_4/ln_1/beta:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma  ->  layer_4/ln_1/gamma:0\n",
      "bert/encoder/layer_4/attention/output/dense/bias  ->  layer_4/c_attn_proj/bias:0\n",
      "bert/encoder/layer_4/attention/output/dense/kernel  ->  layer_4/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_4/attention/self/key/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/key/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/attention/self/query/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/query/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/attention/self/value/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/value/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/intermediate/dense/bias  ->  layer_4/c_fc/bias:0\n",
      "bert/encoder/layer_4/intermediate/dense/kernel  ->  layer_4/c_fc/kernel:0\n",
      "bert/encoder/layer_4/output/LayerNorm/beta  ->  layer_4/ln_2/beta:0\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma  ->  layer_4/ln_2/gamma:0\n",
      "bert/encoder/layer_4/output/dense/bias  ->  layer_4/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_4/output/dense/kernel  ->  layer_4/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta  ->  layer_5/ln_1/beta:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma  ->  layer_5/ln_1/gamma:0\n",
      "bert/encoder/layer_5/attention/output/dense/bias  ->  layer_5/c_attn_proj/bias:0\n",
      "bert/encoder/layer_5/attention/output/dense/kernel  ->  layer_5/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_5/attention/self/key/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/key/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/attention/self/query/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/query/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/attention/self/value/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/value/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/intermediate/dense/bias  ->  layer_5/c_fc/bias:0\n",
      "bert/encoder/layer_5/intermediate/dense/kernel  ->  layer_5/c_fc/kernel:0\n",
      "bert/encoder/layer_5/output/LayerNorm/beta  ->  layer_5/ln_2/beta:0\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma  ->  layer_5/ln_2/gamma:0\n",
      "bert/encoder/layer_5/output/dense/bias  ->  layer_5/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_5/output/dense/kernel  ->  layer_5/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta  ->  layer_6/ln_1/beta:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma  ->  layer_6/ln_1/gamma:0\n",
      "bert/encoder/layer_6/attention/output/dense/bias  ->  layer_6/c_attn_proj/bias:0\n",
      "bert/encoder/layer_6/attention/output/dense/kernel  ->  layer_6/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_6/attention/self/key/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/key/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/attention/self/query/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/query/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/attention/self/value/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/value/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/intermediate/dense/bias  ->  layer_6/c_fc/bias:0\n",
      "bert/encoder/layer_6/intermediate/dense/kernel  ->  layer_6/c_fc/kernel:0\n",
      "bert/encoder/layer_6/output/LayerNorm/beta  ->  layer_6/ln_2/beta:0\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma  ->  layer_6/ln_2/gamma:0\n",
      "bert/encoder/layer_6/output/dense/bias  ->  layer_6/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_6/output/dense/kernel  ->  layer_6/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta  ->  layer_7/ln_1/beta:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma  ->  layer_7/ln_1/gamma:0\n",
      "bert/encoder/layer_7/attention/output/dense/bias  ->  layer_7/c_attn_proj/bias:0\n",
      "bert/encoder/layer_7/attention/output/dense/kernel  ->  layer_7/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_7/attention/self/key/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/key/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/attention/self/query/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/query/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/attention/self/value/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/value/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/intermediate/dense/bias  ->  layer_7/c_fc/bias:0\n",
      "bert/encoder/layer_7/intermediate/dense/kernel  ->  layer_7/c_fc/kernel:0\n",
      "bert/encoder/layer_7/output/LayerNorm/beta  ->  layer_7/ln_2/beta:0\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma  ->  layer_7/ln_2/gamma:0\n",
      "bert/encoder/layer_7/output/dense/bias  ->  layer_7/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_7/output/dense/kernel  ->  layer_7/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/beta  ->  layer_8/ln_1/beta:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/gamma  ->  layer_8/ln_1/gamma:0\n",
      "bert/encoder/layer_8/attention/output/dense/bias  ->  layer_8/c_attn_proj/bias:0\n",
      "bert/encoder/layer_8/attention/output/dense/kernel  ->  layer_8/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_8/attention/self/key/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/key/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/attention/self/query/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/query/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/attention/self/value/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/value/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/intermediate/dense/bias  ->  layer_8/c_fc/bias:0\n",
      "bert/encoder/layer_8/intermediate/dense/kernel  ->  layer_8/c_fc/kernel:0\n",
      "bert/encoder/layer_8/output/LayerNorm/beta  ->  layer_8/ln_2/beta:0\n",
      "bert/encoder/layer_8/output/LayerNorm/gamma  ->  layer_8/ln_2/gamma:0\n",
      "bert/encoder/layer_8/output/dense/bias  ->  layer_8/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_8/output/dense/kernel  ->  layer_8/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/beta  ->  layer_9/ln_1/beta:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/gamma  ->  layer_9/ln_1/gamma:0\n",
      "bert/encoder/layer_9/attention/output/dense/bias  ->  layer_9/c_attn_proj/bias:0\n",
      "bert/encoder/layer_9/attention/output/dense/kernel  ->  layer_9/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_9/attention/self/key/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/key/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/attention/self/query/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/query/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/attention/self/value/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/value/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/intermediate/dense/bias  ->  layer_9/c_fc/bias:0\n",
      "bert/encoder/layer_9/intermediate/dense/kernel  ->  layer_9/c_fc/kernel:0\n",
      "bert/encoder/layer_9/output/LayerNorm/beta  ->  layer_9/ln_2/beta:0\n",
      "bert/encoder/layer_9/output/LayerNorm/gamma  ->  layer_9/ln_2/gamma:0\n",
      "bert/encoder/layer_9/output/dense/bias  ->  layer_9/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_9/output/dense/kernel  ->  layer_9/c_ffn_proj/kernel:0\n",
      "not mapped:  bert/pooler/dense/bias\n",
      "not mapped:  bert/pooler/dense/kernel\n",
      "not mapped:  cls/predictions/output_bias\n",
      "not mapped:  cls/predictions/transform/LayerNorm/beta\n",
      "not mapped:  cls/predictions/transform/LayerNorm/gamma\n",
      "not mapped:  cls/predictions/transform/dense/bias\n",
      "not mapped:  cls/predictions/transform/dense/kernel\n",
      "not mapped:  cls/seq_relationship/output_bias\n",
      "not mapped:  cls/seq_relationship/output_weights\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 70, 768)      1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 70, 768)      393216      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 70, 768)      91812096    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 70, 768)      0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 70, 768)      1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 70, 768)      0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 70, 2304)     1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 70, 768)      0           layer_0/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 70, 768)      0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 70, 3072)     0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 70, 768)      0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 70, 768)      0           layer_1/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 70, 768)      0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 70, 3072)     0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 70, 768)      0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 70, 768)      0           layer_2/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 70, 768)      0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 70, 3072)     0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 70, 768)      0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 70, 768)      0           layer_3/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 70, 768)      0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 70, 3072)     0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 70, 768)      0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 70, 768)      0           layer_4/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 70, 768)      0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 70, 3072)     0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 70, 768)      0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 70, 768)      0           layer_5/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 70, 768)      0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 70, 3072)     0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 70, 768)      0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 70, 768)      0           layer_6/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 70, 768)      0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 70, 3072)     0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 70, 768)      0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 70, 768)      0           layer_7/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 70, 768)      0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 70, 3072)     0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 70, 768)      0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 70, 768)      0           layer_8/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 70, 768)      0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 70, 3072)     0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 70, 768)      0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 70, 768)      0           layer_9/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 70, 768)      0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 70, 3072)     0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_9/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 70, 768)      0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 70, 768)      0           layer_10/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 70, 768)      0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 70, 3072)     0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 70, 768)      0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 70, 768)      0           layer_11/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 70, 768)      0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 70, 3072)     0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 70, 768)      0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_2_add[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 177,262,848\n",
      "Trainable params: 177,262,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g_bert = load_google_bert(base_location=BERT_PRETRAINED_DIR, use_attn_mask=False, max_len=SEQ_LEN)\n",
    "g_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Layer 0 as containing the features relevant for classification; see BERT paper for further explanation on\n",
    "# this choice.\n",
    "classification_features = Lambda(lambda x: x[:, 0, :])(g_bert.output)\n",
    "out = Dense(3, activation='softmax')(classification_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 70, 768)      1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 70, 768)      393216      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 70, 768)      91812096    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 70, 768)      0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 70, 768)      1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 70, 768)      0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 70, 2304)     1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 70, 768)      0           layer_0/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 70, 768)      0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 70, 3072)     0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 70, 768)      0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 70, 768)      0           layer_1/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 70, 768)      0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 70, 3072)     0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 70, 768)      0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 70, 768)      0           layer_2/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 70, 768)      0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 70, 3072)     0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 70, 768)      0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 70, 768)      0           layer_3/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 70, 768)      0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 70, 3072)     0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 70, 768)      0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 70, 768)      0           layer_4/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 70, 768)      0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 70, 3072)     0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 70, 768)      0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 70, 768)      0           layer_5/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 70, 768)      0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 70, 3072)     0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 70, 768)      0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 70, 768)      0           layer_6/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 70, 768)      0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 70, 3072)     0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 70, 768)      0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 70, 768)      0           layer_7/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 70, 768)      0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 70, 3072)     0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 70, 768)      0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 70, 768)      0           layer_8/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 70, 768)      0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 70, 3072)     0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 70, 768)      0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 70, 768)      0           layer_9/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 70, 768)      0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 70, 3072)     0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 70, 768)      0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 70, 768)      0           layer_10/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 70, 768)      0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 70, 3072)     0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 70, 768)      0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 70, 768)      0           layer_11/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 70, 768)      0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 70, 3072)     0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 70, 768)      0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            2307        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 177,265,155\n",
      "Trainable params: 177,265,155\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(g_bert.inputs, out)\n",
    "model.compile(optimizer=Adam(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-da1510e38b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=valid_gen,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid_predictions = model.predict_generator(valid_gen, verbose=1)\n",
    "Y_test = Y_test[:len(Y_valid_predictions)]\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    f1 = metrics.f1_score(Y_test, (Y_valid_predictions > thresh).astype(int))\n",
    "    print(f\"F1 score at threshold {thresh} is {f1}\")\n",
    "\n",
    "'''\n",
    "After 1 Epoch\n",
    "F1 score at threshold 0.32 is 0.687372802960222\n",
    "Note that the results may vary slightly from run to run due to the non-deterministic nature of tensorflow/keras.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
