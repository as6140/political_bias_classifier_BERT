{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Bias in Tweets by Members of Congress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributors: Alex Shropshire, Mando Iwanaga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Use transfer learning (Pre-trained BERT model) to classify tweet text from Politicians as having a democratic bias, a republican bias, or neutrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Business Understanding  \n",
    "2.Understand the data  \n",
    "3.Prepare the data for analysis and modeling  \n",
    "4.Model  \n",
    "5.Evaluate Results  \n",
    "6.Deploy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from batch_generator.batch_generator import BatchGenerator\n",
    "from load_pretrained_bert import load_google_bert\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll begin by uploading our dataset retrieved from [figure-eight](https://www.figure-eight.com/data-for-everyone/), an open source data platform.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                              label   source  \\\n",
       "0    From: Trey Radel (Representative from Florida)  twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload dataset with only the necessary columns\n",
    "raw_df = pd.read_csv('Political-media-DFE.csv',encoding='latin')\n",
    "df = raw_df[['bias','message','embed','label','source','text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3689\n",
       "partisan    1311\n",
       "Name: bias, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset does not specify political affiliation, will need to add politician's affiliations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to clean text of puntuations\n",
    "def remove_punctuations(text):\n",
    "    '''Removes punctuation from strings'''\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Apply our remove_punctuations function\n",
    "df['text'] = df.loc[:,'text'].apply(remove_punctuations)\n",
    "df['label'] = df['label'].str.replace('From: ','')\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Create a new column with the tweet purpose and bias\n",
    "df['purpose_and_bias'] = df['message'] + '_' + df['bias']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan  \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan  \n",
       "2  please join me today in remembering our fallen...  support_neutral  \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral  \n",
       "4  amazon delivery drones show need to update law...  policy_partisan  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gregorio</td>\n",
       "      <td>Sablan</td>\n",
       "      <td>Gregorio Sablan (Representative from NA)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>Robert Aderholt (Representative from Alabama)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lamar</td>\n",
       "      <td>Alexander</td>\n",
       "      <td>Lamar Alexander (Senator from Tennessee)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Justin</td>\n",
       "      <td>Amash</td>\n",
       "      <td>Justin Amash (Representative from Michigan)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark</td>\n",
       "      <td>Amodei</td>\n",
       "      <td>Mark Amodei (Representative from Nevada)</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      First       Last                                    congressman  \\\n",
       "0  Gregorio     Sablan       Gregorio Sablan (Representative from NA)   \n",
       "1    Robert   Aderholt  Robert Aderholt (Representative from Alabama)   \n",
       "2     Lamar  Alexander       Lamar Alexander (Senator from Tennessee)   \n",
       "3    Justin      Amash    Justin Amash (Representative from Michigan)   \n",
       "4      Mark     Amodei       Mark Amodei (Representative from Nevada)   \n",
       "\n",
       "  affiliation  \n",
       "0           d  \n",
       "1           r  \n",
       "2           r  \n",
       "3           r  \n",
       "4           r  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset of politicians and their affiliations\n",
    "congressmen_df = pd.read_csv('congressmen_2015.csv')\n",
    "congressmen_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the two datasets to include politicians affiliations and our target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partisan tweets will be labeled as the politicians affiliation in target column\n",
    "#neutral tweets will be labeled as neutral in target column\n",
    "df = df.merge(congressmen_df, how='left',left_on='label',right_on='congressman')\n",
    "df.loc[df.bias == 'partisan', 'target'] = df['affiliation']\n",
    "df.loc[df.bias == 'neutral', 'target'] = df['bias']\n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with target label as \"i\" \n",
    "df = df[df['target'] != 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Trey</td>\n",
       "      <td>Radel</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>r</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Schrader</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>d</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Crapo</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>r</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Udall</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \\\n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan   \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan   \n",
       "2  please join me today in remembering our fallen...  support_neutral   \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral   \n",
       "4  amazon delivery drones show need to update law...  policy_partisan   \n",
       "\n",
       "     First       Last                                 congressman affiliation  \\\n",
       "0     Trey      Radel    Trey Radel (Representative from Florida)           r   \n",
       "1    Mitch  McConnell     Mitch McConnell (Senator from Kentucky)           r   \n",
       "2     Kurt   Schrader  Kurt Schrader (Representative from Oregon)           d   \n",
       "3  Michael      Crapo          Michael Crapo (Senator from Idaho)           r   \n",
       "4     Mark      Udall          Mark Udall (Senator from Colorado)           d   \n",
       "\n",
       "    target  \n",
       "0        r  \n",
       "1        r  \n",
       "2  neutral  \n",
       "3  neutral  \n",
       "4        d  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    3631\n",
       "r           791\n",
       "d           490\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have 3 target classifications\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll create functions to clean our text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contraction(text):\n",
    "    \"\"\"Replace contractions from text\"\"\"\n",
    "    contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'can not'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_links(text, filler=' '):\n",
    "    \"\"\"Replace url links included in text\"\"\"\n",
    "    text = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*',\n",
    "                      filler, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\"Remove numbers from text\"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to incorporate three functions above in one\n",
    "def cleanText(text):\n",
    "    \"\"\"Incorporate three created functions above into one\"\"\"\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = replace_contraction(text)\n",
    "    text = replace_links(text, \"link\")\n",
    "    text = remove_numbers(text)\n",
    "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{]',\"\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label our target variables in single column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>embed</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>purpose_and_bias</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>congressman</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt nowthisnews rep trey radel r fl slams obama...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Trey</td>\n",
       "      <td>Radel</td>\n",
       "      <td>Trey Radel (Representative from Florida)</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>video  obamacare  full of higher costs and bro...</td>\n",
       "      <td>attack_partisan</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>McConnell</td>\n",
       "      <td>Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>please join me today in remembering our fallen...</td>\n",
       "      <td>support_neutral</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Schrader</td>\n",
       "      <td>Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>rt senatorleahy 1st step toward senate debate ...</td>\n",
       "      <td>policy_neutral</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Crapo</td>\n",
       "      <td>Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>r</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>amazon delivery drones show need to update law...</td>\n",
       "      <td>policy_partisan</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Udall</td>\n",
       "      <td>Mark Udall (Senator from Colorado)</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  message                                              embed  \\\n",
       "0  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "1  partisan   attack  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "2   neutral  support  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "3   neutral   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "4  partisan   policy  <blockquote class=\"twitter-tweet\" width=\"450\">...   \n",
       "\n",
       "                                        label   source  \\\n",
       "0    Trey Radel (Representative from Florida)  twitter   \n",
       "1     Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "2  Kurt Schrader (Representative from Oregon)  twitter   \n",
       "3          Michael Crapo (Senator from Idaho)  twitter   \n",
       "4          Mark Udall (Senator from Colorado)  twitter   \n",
       "\n",
       "                                                text purpose_and_bias  \\\n",
       "0  rt nowthisnews rep trey radel r fl slams obama...  policy_partisan   \n",
       "1  video  obamacare  full of higher costs and bro...  attack_partisan   \n",
       "2  please join me today in remembering our fallen...  support_neutral   \n",
       "3  rt senatorleahy 1st step toward senate debate ...   policy_neutral   \n",
       "4  amazon delivery drones show need to update law...  policy_partisan   \n",
       "\n",
       "     First       Last                                 congressman affiliation  \\\n",
       "0     Trey      Radel    Trey Radel (Representative from Florida)           r   \n",
       "1    Mitch  McConnell     Mitch McConnell (Senator from Kentucky)           r   \n",
       "2     Kurt   Schrader  Kurt Schrader (Representative from Oregon)           d   \n",
       "3  Michael      Crapo          Michael Crapo (Senator from Idaho)           r   \n",
       "4     Mark      Udall          Mark Udall (Senator from Colorado)           d   \n",
       "\n",
       "  target  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['target'] == 'neutral', 'target'] = 0\n",
    "df.loc[df['target'] == 'r', 'target'] = 1\n",
    "df.loc[df['target'] == 'd', 'target'] = 2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip multi_cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload pretrained BERT model\n",
    "BERT_PRETRAINED_DIR = 'multi_cased_L-12_H-768_A-12/'\n",
    "SEQ_LEN = 70\n",
    "BATCH_SIZE = 12\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt nowthisnews rep trey radel r fl slams obamacare politics httpstcozvywmgyih\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Define our X and Y variables and apply cleanText function\n",
    "X = df['text'].apply(cleanText).values\n",
    "Y = df['target'].values\n",
    "print(X[0])  \n",
    "print(Y[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split our data into training sets and test sets using default parameters\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0621 17:58:44.618396 139777946162944 deprecation_wrapper.py:119] From /home/jupyter/political_bias_classifier_BERT/batch_generator/tokenization.py:74: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3684/3684 [00:02<00:00, 1756.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3684/3684 [00:00<00:00, 95100.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1228/1228 [00:00<00:00, 1840.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1228/1228 [00:00<00:00, 103619.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#Define batch generators\n",
    "train_gen = BatchGenerator(X_train,\n",
    "                           vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                           seq_len=SEQ_LEN,\n",
    "                           labels=Y_train,\n",
    "                           do_lower_case=False,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "valid_gen = BatchGenerator(X_test,\n",
    "                           vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                           seq_len=SEQ_LEN,\n",
    "                           labels=Y_test,\n",
    "                           do_lower_case=False,\n",
    "                           batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 17:58:51.940794 139777946162944 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0621 17:58:51.983125 139777946162944 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0621 17:58:52.003972 139777946162944 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0621 17:58:52.047201 139777946162944 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0621 17:58:52.055370 139777946162944 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0621 17:58:52.197570 139777946162944 deprecation_wrapper.py:119] From /home/jupyter/political_bias_classifier_BERT/transformer/layers.py:75: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/embeddings/LayerNorm/beta  ->  layer_normalization_1/beta:0\n",
      "bert/embeddings/LayerNorm/gamma  ->  layer_normalization_1/gamma:0\n",
      "bert/embeddings/position_embeddings  ->  PositionEmbedding/embeddings:0\n",
      "bert/embeddings/token_type_embeddings  ->  SegmentEmbedding/embeddings:0\n",
      "bert/embeddings/word_embeddings  ->  TokenEmbedding/embeddings:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta  ->  layer_0/ln_1/beta:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma  ->  layer_0/ln_1/gamma:0\n",
      "bert/encoder/layer_0/attention/output/dense/bias  ->  layer_0/c_attn_proj/bias:0\n",
      "bert/encoder/layer_0/attention/output/dense/kernel  ->  layer_0/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_0/attention/self/key/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/key/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/attention/self/query/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/query/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/attention/self/value/bias  ->  layer_0/c_attn/bias:0\n",
      "bert/encoder/layer_0/attention/self/value/kernel  ->  layer_0/c_attn/kernel:0\n",
      "bert/encoder/layer_0/intermediate/dense/bias  ->  layer_0/c_fc/bias:0\n",
      "bert/encoder/layer_0/intermediate/dense/kernel  ->  layer_0/c_fc/kernel:0\n",
      "bert/encoder/layer_0/output/LayerNorm/beta  ->  layer_0/ln_2/beta:0\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma  ->  layer_0/ln_2/gamma:0\n",
      "bert/encoder/layer_0/output/dense/bias  ->  layer_0/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_0/output/dense/kernel  ->  layer_0/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta  ->  layer_1/ln_1/beta:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma  ->  layer_1/ln_1/gamma:0\n",
      "bert/encoder/layer_1/attention/output/dense/bias  ->  layer_1/c_attn_proj/bias:0\n",
      "bert/encoder/layer_1/attention/output/dense/kernel  ->  layer_1/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_1/attention/self/key/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/key/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/attention/self/query/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/query/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/attention/self/value/bias  ->  layer_1/c_attn/bias:0\n",
      "bert/encoder/layer_1/attention/self/value/kernel  ->  layer_1/c_attn/kernel:0\n",
      "bert/encoder/layer_1/intermediate/dense/bias  ->  layer_1/c_fc/bias:0\n",
      "bert/encoder/layer_1/intermediate/dense/kernel  ->  layer_1/c_fc/kernel:0\n",
      "bert/encoder/layer_1/output/LayerNorm/beta  ->  layer_1/ln_2/beta:0\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma  ->  layer_1/ln_2/gamma:0\n",
      "bert/encoder/layer_1/output/dense/bias  ->  layer_1/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_1/output/dense/kernel  ->  layer_1/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/beta  ->  layer_10/ln_1/beta:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/gamma  ->  layer_10/ln_1/gamma:0\n",
      "bert/encoder/layer_10/attention/output/dense/bias  ->  layer_10/c_attn_proj/bias:0\n",
      "bert/encoder/layer_10/attention/output/dense/kernel  ->  layer_10/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_10/attention/self/key/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/key/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/attention/self/query/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/query/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/attention/self/value/bias  ->  layer_10/c_attn/bias:0\n",
      "bert/encoder/layer_10/attention/self/value/kernel  ->  layer_10/c_attn/kernel:0\n",
      "bert/encoder/layer_10/intermediate/dense/bias  ->  layer_10/c_fc/bias:0\n",
      "bert/encoder/layer_10/intermediate/dense/kernel  ->  layer_10/c_fc/kernel:0\n",
      "bert/encoder/layer_10/output/LayerNorm/beta  ->  layer_10/ln_2/beta:0\n",
      "bert/encoder/layer_10/output/LayerNorm/gamma  ->  layer_10/ln_2/gamma:0\n",
      "bert/encoder/layer_10/output/dense/bias  ->  layer_10/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_10/output/dense/kernel  ->  layer_10/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/beta  ->  layer_11/ln_1/beta:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/gamma  ->  layer_11/ln_1/gamma:0\n",
      "bert/encoder/layer_11/attention/output/dense/bias  ->  layer_11/c_attn_proj/bias:0\n",
      "bert/encoder/layer_11/attention/output/dense/kernel  ->  layer_11/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_11/attention/self/key/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/key/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/attention/self/query/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/query/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/attention/self/value/bias  ->  layer_11/c_attn/bias:0\n",
      "bert/encoder/layer_11/attention/self/value/kernel  ->  layer_11/c_attn/kernel:0\n",
      "bert/encoder/layer_11/intermediate/dense/bias  ->  layer_11/c_fc/bias:0\n",
      "bert/encoder/layer_11/intermediate/dense/kernel  ->  layer_11/c_fc/kernel:0\n",
      "bert/encoder/layer_11/output/LayerNorm/beta  ->  layer_11/ln_2/beta:0\n",
      "bert/encoder/layer_11/output/LayerNorm/gamma  ->  layer_11/ln_2/gamma:0\n",
      "bert/encoder/layer_11/output/dense/bias  ->  layer_11/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_11/output/dense/kernel  ->  layer_11/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta  ->  layer_2/ln_1/beta:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma  ->  layer_2/ln_1/gamma:0\n",
      "bert/encoder/layer_2/attention/output/dense/bias  ->  layer_2/c_attn_proj/bias:0\n",
      "bert/encoder/layer_2/attention/output/dense/kernel  ->  layer_2/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_2/attention/self/key/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/key/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/attention/self/query/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/query/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/attention/self/value/bias  ->  layer_2/c_attn/bias:0\n",
      "bert/encoder/layer_2/attention/self/value/kernel  ->  layer_2/c_attn/kernel:0\n",
      "bert/encoder/layer_2/intermediate/dense/bias  ->  layer_2/c_fc/bias:0\n",
      "bert/encoder/layer_2/intermediate/dense/kernel  ->  layer_2/c_fc/kernel:0\n",
      "bert/encoder/layer_2/output/LayerNorm/beta  ->  layer_2/ln_2/beta:0\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma  ->  layer_2/ln_2/gamma:0\n",
      "bert/encoder/layer_2/output/dense/bias  ->  layer_2/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_2/output/dense/kernel  ->  layer_2/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta  ->  layer_3/ln_1/beta:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma  ->  layer_3/ln_1/gamma:0\n",
      "bert/encoder/layer_3/attention/output/dense/bias  ->  layer_3/c_attn_proj/bias:0\n",
      "bert/encoder/layer_3/attention/output/dense/kernel  ->  layer_3/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_3/attention/self/key/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/key/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/attention/self/query/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/query/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/attention/self/value/bias  ->  layer_3/c_attn/bias:0\n",
      "bert/encoder/layer_3/attention/self/value/kernel  ->  layer_3/c_attn/kernel:0\n",
      "bert/encoder/layer_3/intermediate/dense/bias  ->  layer_3/c_fc/bias:0\n",
      "bert/encoder/layer_3/intermediate/dense/kernel  ->  layer_3/c_fc/kernel:0\n",
      "bert/encoder/layer_3/output/LayerNorm/beta  ->  layer_3/ln_2/beta:0\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma  ->  layer_3/ln_2/gamma:0\n",
      "bert/encoder/layer_3/output/dense/bias  ->  layer_3/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_3/output/dense/kernel  ->  layer_3/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/beta  ->  layer_4/ln_1/beta:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma  ->  layer_4/ln_1/gamma:0\n",
      "bert/encoder/layer_4/attention/output/dense/bias  ->  layer_4/c_attn_proj/bias:0\n",
      "bert/encoder/layer_4/attention/output/dense/kernel  ->  layer_4/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_4/attention/self/key/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/key/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/attention/self/query/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/query/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/attention/self/value/bias  ->  layer_4/c_attn/bias:0\n",
      "bert/encoder/layer_4/attention/self/value/kernel  ->  layer_4/c_attn/kernel:0\n",
      "bert/encoder/layer_4/intermediate/dense/bias  ->  layer_4/c_fc/bias:0\n",
      "bert/encoder/layer_4/intermediate/dense/kernel  ->  layer_4/c_fc/kernel:0\n",
      "bert/encoder/layer_4/output/LayerNorm/beta  ->  layer_4/ln_2/beta:0\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma  ->  layer_4/ln_2/gamma:0\n",
      "bert/encoder/layer_4/output/dense/bias  ->  layer_4/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_4/output/dense/kernel  ->  layer_4/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta  ->  layer_5/ln_1/beta:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma  ->  layer_5/ln_1/gamma:0\n",
      "bert/encoder/layer_5/attention/output/dense/bias  ->  layer_5/c_attn_proj/bias:0\n",
      "bert/encoder/layer_5/attention/output/dense/kernel  ->  layer_5/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_5/attention/self/key/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/key/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/attention/self/query/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/query/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/attention/self/value/bias  ->  layer_5/c_attn/bias:0\n",
      "bert/encoder/layer_5/attention/self/value/kernel  ->  layer_5/c_attn/kernel:0\n",
      "bert/encoder/layer_5/intermediate/dense/bias  ->  layer_5/c_fc/bias:0\n",
      "bert/encoder/layer_5/intermediate/dense/kernel  ->  layer_5/c_fc/kernel:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/encoder/layer_5/output/LayerNorm/beta  ->  layer_5/ln_2/beta:0\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma  ->  layer_5/ln_2/gamma:0\n",
      "bert/encoder/layer_5/output/dense/bias  ->  layer_5/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_5/output/dense/kernel  ->  layer_5/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta  ->  layer_6/ln_1/beta:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma  ->  layer_6/ln_1/gamma:0\n",
      "bert/encoder/layer_6/attention/output/dense/bias  ->  layer_6/c_attn_proj/bias:0\n",
      "bert/encoder/layer_6/attention/output/dense/kernel  ->  layer_6/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_6/attention/self/key/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/key/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/attention/self/query/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/query/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/attention/self/value/bias  ->  layer_6/c_attn/bias:0\n",
      "bert/encoder/layer_6/attention/self/value/kernel  ->  layer_6/c_attn/kernel:0\n",
      "bert/encoder/layer_6/intermediate/dense/bias  ->  layer_6/c_fc/bias:0\n",
      "bert/encoder/layer_6/intermediate/dense/kernel  ->  layer_6/c_fc/kernel:0\n",
      "bert/encoder/layer_6/output/LayerNorm/beta  ->  layer_6/ln_2/beta:0\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma  ->  layer_6/ln_2/gamma:0\n",
      "bert/encoder/layer_6/output/dense/bias  ->  layer_6/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_6/output/dense/kernel  ->  layer_6/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta  ->  layer_7/ln_1/beta:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma  ->  layer_7/ln_1/gamma:0\n",
      "bert/encoder/layer_7/attention/output/dense/bias  ->  layer_7/c_attn_proj/bias:0\n",
      "bert/encoder/layer_7/attention/output/dense/kernel  ->  layer_7/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_7/attention/self/key/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/key/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/attention/self/query/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/query/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/attention/self/value/bias  ->  layer_7/c_attn/bias:0\n",
      "bert/encoder/layer_7/attention/self/value/kernel  ->  layer_7/c_attn/kernel:0\n",
      "bert/encoder/layer_7/intermediate/dense/bias  ->  layer_7/c_fc/bias:0\n",
      "bert/encoder/layer_7/intermediate/dense/kernel  ->  layer_7/c_fc/kernel:0\n",
      "bert/encoder/layer_7/output/LayerNorm/beta  ->  layer_7/ln_2/beta:0\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma  ->  layer_7/ln_2/gamma:0\n",
      "bert/encoder/layer_7/output/dense/bias  ->  layer_7/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_7/output/dense/kernel  ->  layer_7/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/beta  ->  layer_8/ln_1/beta:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/gamma  ->  layer_8/ln_1/gamma:0\n",
      "bert/encoder/layer_8/attention/output/dense/bias  ->  layer_8/c_attn_proj/bias:0\n",
      "bert/encoder/layer_8/attention/output/dense/kernel  ->  layer_8/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_8/attention/self/key/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/key/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/attention/self/query/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/query/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/attention/self/value/bias  ->  layer_8/c_attn/bias:0\n",
      "bert/encoder/layer_8/attention/self/value/kernel  ->  layer_8/c_attn/kernel:0\n",
      "bert/encoder/layer_8/intermediate/dense/bias  ->  layer_8/c_fc/bias:0\n",
      "bert/encoder/layer_8/intermediate/dense/kernel  ->  layer_8/c_fc/kernel:0\n",
      "bert/encoder/layer_8/output/LayerNorm/beta  ->  layer_8/ln_2/beta:0\n",
      "bert/encoder/layer_8/output/LayerNorm/gamma  ->  layer_8/ln_2/gamma:0\n",
      "bert/encoder/layer_8/output/dense/bias  ->  layer_8/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_8/output/dense/kernel  ->  layer_8/c_ffn_proj/kernel:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/beta  ->  layer_9/ln_1/beta:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/gamma  ->  layer_9/ln_1/gamma:0\n",
      "bert/encoder/layer_9/attention/output/dense/bias  ->  layer_9/c_attn_proj/bias:0\n",
      "bert/encoder/layer_9/attention/output/dense/kernel  ->  layer_9/c_attn_proj/kernel:0\n",
      "bert/encoder/layer_9/attention/self/key/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/key/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/attention/self/query/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/query/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/attention/self/value/bias  ->  layer_9/c_attn/bias:0\n",
      "bert/encoder/layer_9/attention/self/value/kernel  ->  layer_9/c_attn/kernel:0\n",
      "bert/encoder/layer_9/intermediate/dense/bias  ->  layer_9/c_fc/bias:0\n",
      "bert/encoder/layer_9/intermediate/dense/kernel  ->  layer_9/c_fc/kernel:0\n",
      "bert/encoder/layer_9/output/LayerNorm/beta  ->  layer_9/ln_2/beta:0\n",
      "bert/encoder/layer_9/output/LayerNorm/gamma  ->  layer_9/ln_2/gamma:0\n",
      "bert/encoder/layer_9/output/dense/bias  ->  layer_9/c_ffn_proj/bias:0\n",
      "bert/encoder/layer_9/output/dense/kernel  ->  layer_9/c_ffn_proj/kernel:0\n",
      "not mapped:  bert/pooler/dense/bias\n",
      "not mapped:  bert/pooler/dense/kernel\n",
      "not mapped:  cls/predictions/output_bias\n",
      "not mapped:  cls/predictions/transform/LayerNorm/beta\n",
      "not mapped:  cls/predictions/transform/LayerNorm/gamma\n",
      "not mapped:  cls/predictions/transform/dense/bias\n",
      "not mapped:  cls/predictions/transform/dense/kernel\n",
      "not mapped:  cls/seq_relationship/output_bias\n",
      "not mapped:  cls/seq_relationship/output_weights\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 70, 768)      1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 70, 768)      393216      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 70, 768)      91812096    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 70, 768)      0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 70, 768)      1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 70, 768)      0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 70, 2304)     1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 70, 768)      0           layer_0/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 70, 768)      0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 70, 3072)     0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 70, 768)      0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 70, 768)      0           layer_1/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 70, 768)      0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 70, 3072)     0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 70, 768)      0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 70, 768)      0           layer_2/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 70, 768)      0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 70, 3072)     0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 70, 768)      0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 70, 768)      0           layer_3/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 70, 768)      0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 70, 3072)     0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 70, 768)      0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 70, 768)      0           layer_4/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 70, 768)      0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 70, 3072)     0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 70, 768)      0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 70, 768)      0           layer_5/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 70, 768)      0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 70, 3072)     0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 70, 768)      0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 70, 768)      0           layer_6/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 70, 768)      0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 70, 3072)     0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 70, 768)      0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 70, 768)      0           layer_7/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 70, 768)      0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 70, 3072)     0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 70, 768)      0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 70, 768)      0           layer_8/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 70, 768)      0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 70, 3072)     0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 70, 768)      0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 70, 768)      0           layer_9/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 70, 768)      0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 70, 3072)     0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 70, 768)      0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 70, 768)      0           layer_10/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 70, 768)      0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 70, 3072)     0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 70, 768)      0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 70, 768)      0           layer_11/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 70, 768)      0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 70, 3072)     0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 70, 768)      0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_2_add[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 177,262,848\n",
      "Trainable params: 177,262,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Load BERT pretrained model and print summary\n",
    "g_bert = load_google_bert(base_location=BERT_PRETRAINED_DIR, use_attn_mask=False, max_len=SEQ_LEN)\n",
    "g_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Layer 0 as containing the features relevant for classification; see BERT paper for further explanation on\n",
    "# this choice.\n",
    "classification_features = Lambda(lambda x: x[:, 0, :])(g_bert.output)\n",
    "out = Dense(3, activation='softmax')(classification_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 70, 768)      1536        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 70, 768)      393216      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 70, 768)      91812096    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 70, 768)      0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 70, 768)      1536        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 70, 768)      0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 70, 2304)     1771776     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 70, 768)      0           layer_0/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 70, 768)      0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 70, 3072)     0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 70, 768)      0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 70, 768)      0           layer_1/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 70, 768)      0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 70, 3072)     0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 70, 768)      0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 70, 768)      0           layer_2/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 70, 768)      0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 70, 3072)     0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 70, 768)      0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 70, 768)      0           layer_3/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 70, 768)      0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 70, 3072)     0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 70, 768)      0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 70, 768)      0           layer_4/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 70, 768)      0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 70, 3072)     0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 70, 768)      0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 70, 768)      0           layer_5/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 70, 768)      0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 70, 3072)     0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 70, 768)      0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 70, 768)      0           layer_6/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 70, 768)      0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 70, 3072)     0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 70, 768)      0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 70, 768)      0           layer_7/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 70, 768)      0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 70, 3072)     0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 70, 768)      0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 70, 768)      0           layer_8/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 70, 768)      0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 70, 3072)     0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 70, 768)      0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 70, 2304)     1771776     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 70, 768)      0           layer_9/c_attn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 70, 768)      590592      layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 70, 768)      0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 70, 768)      0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 70, 3072)     2362368     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 70, 3072)     0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 70, 768)      2360064     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 70, 768)      0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 70, 768)      0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 70, 768)      1536        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 70, 768)      0           layer_10/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 70, 768)      0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 70, 3072)     0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 70, 768)      0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 70, 2304)     1771776     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 70, 768)      0           layer_11/c_attn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 70, 768)      590592      layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 70, 768)      0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 70, 768)      0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 70, 3072)     2362368     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 70, 3072)     0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 70, 768)      2360064     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 70, 768)      0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 70, 768)      0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 70, 768)      1536        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 768)          0           layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            2307        lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 177,265,155\n",
      "Trainable params: 177,265,155\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define model, compile, and define parameters\n",
    "model = Model(g_bert.inputs, out)\n",
    "model.compile(optimizer=Adam(LR), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will fit our model and incorporate Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "307/307 [==============================] - 133s 435ms/step - loss: 0.8249 - acc: 0.7324 - val_loss: 0.7734 - val_acc: 0.7328\n",
      "Epoch 2/20\n",
      "307/307 [==============================] - 114s 373ms/step - loss: 0.7695 - acc: 0.7408 - val_loss: 0.8217 - val_acc: 0.7328\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-08, patience=0, verbose=1, mode='auto')\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "history_log = model.fit_generator(train_gen,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=valid_gen,\n",
    "                    shuffle=True, \n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 9s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_test_predictions = model.predict_generator(valid_gen, verbose=1)\n",
    "Y_test = Y_test[:len(Y_test_predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7368598 , 0.18633462, 0.07680555],\n",
       "       [0.7368598 , 0.18633462, 0.07680555],\n",
       "       [0.7368598 , 0.18633462, 0.07680555],\n",
       "       [0.7368598 , 0.18633461, 0.07680552],\n",
       "       [0.73685986, 0.18633462, 0.07680551]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import sparse_categorical_accuracy, categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-1daf98fdcfee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36mcategorical_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     return K.cast(K.equal(K.argmax(y_true, axis=-1),\n\u001b[0m\u001b[1;32m     32\u001b[0m                           K.argmax(y_pred, axis=-1)),\n\u001b[1;32m     33\u001b[0m                   K.floatx())\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \"\"\"\n\u001b[0;32m-> 1445\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(input, axis, name, dimension, output_type)\u001b[0m\n\u001b[1;32m    136\u001b[0m   axis = deprecation.deprecated_argument_lookup(\"axis\", axis, \"dimension\",\n\u001b[1;32m    137\u001b[0m                                                 dimension)\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0margmax_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36margmax_v2\u001b[0;34m(input, axis, output_type, name)\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36marg_max\u001b[0;34m(input, dimension, output_type, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m    947\u001b[0m         \u001b[0;34m\"ArgMax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m    949\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    528\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m               raise TypeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    528\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    283\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    571\u001b[0m     raise TypeError(\n\u001b[1;32m    572\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 573\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got 0"
     ]
    }
   ],
   "source": [
    "categorical_accuracy(Y_test, Y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-00764454b26b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_categorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36msparse_categorical_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_categorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     return K.cast(K.equal(K.flatten(y_true),\n\u001b[0m\u001b[1;32m     39\u001b[0m                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n\u001b[1;32m     40\u001b[0m                   K.floatx())\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2201\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped\u001b[0m \u001b[0minto\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \"\"\"\n\u001b[0;32m-> 2203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7713\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7714\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 7715\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   7716\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7717\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    528\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m               raise TypeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    528\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                          as_ref=False):\n\u001b[1;32m    304\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   \"\"\"\n\u001b[1;32m    245\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 246\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m    283\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    571\u001b[0m     raise TypeError(\n\u001b[1;32m    572\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 573\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got 0"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy(Y_test, Y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-e03b07d605a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print ({f1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "# for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "#     thresh = np.round(thresh, 2)\n",
    "#     f1 = metrics.f1_score(Y_test, (Y_test_predictions > thresh).astype(int), average='micro')\n",
    "#     print ({thresh})\n",
    "#     print ({f1})\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_test_predictions)\n",
    "recall = recall_score(Y_test, Y_test_predictions, average='micro')\n",
    "precision = precision_score(Y_test, Y_test_predictions, average='micro')\n",
    "f1 = f1_score(Y_test, Y_test_predictions, average='micro')\n",
    "\n",
    "#print('The calculated p-value is {}'.format( round(p_val, 90) ))\n",
    "print (\"Accuracy: {}\".format(accuracy))\n",
    "print (\"Recall: {}\".format(recall))\n",
    "print (\"Precision: {}\".format(precision))\n",
    "print (\"F-1 Score: {}\".format(f1_score))\n",
    "\n",
    "\n",
    "'''\n",
    "After 1 Epoch\n",
    "F1 score at threshold 0.32 is 0.687372802960222\n",
    "Note that the results may vary slightly from run to run due to the non-deterministic nature of tensorflow/keras.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-d8a926b4a35c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \"\"\"\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "classification_report(Y_test, Y_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_log.history['loss'], label = 'train loss')\n",
    "plt.plot(history_log.history['val_loss'], label = 'test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-122bc20ff125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(f\"Log loss: {log_loss(Y_test, Y_test_predictions)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(roc_auc_score(Y_test, Y_test_predictions[:,1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ROC curve'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \"\"\"\n\u001b[1;32m    621\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 622\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    394\u001b[0m     if not (y_type == \"binary\" or\n\u001b[1;32m    395\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown format is not supported"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "#print(f\"Log loss: {log_loss(Y_test, Y_test_predictions)}\")\n",
    "#print(roc_auc_score(Y_test, Y_test_predictions[:,1]))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_test_predictions[:,1])\n",
    "# Plot\n",
    "plt.plot(fpr, tpr, label='ROC curve')\n",
    "\n",
    "# This is just a diagonal\n",
    "plt.plot([0, 1], [0, 1], linestyle='--');\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Receiver operating characteristic (ROC) Curve', fontsize=15)\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate scores\n",
    "#plot loss\n",
    "#plot auc curve\n",
    "#average='micro'\n",
    "\n",
    "# update slides\n",
    "#TEST sample tweets?\n",
    "# update repo / README\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8559505656021814, 0.7662766807339478, 0.7602843923997802],\n",
       " 'sparse_categorical_accuracy': [0.7334419113950932,\n",
       "  0.7410423457622528,\n",
       "  0.7410423457622528],\n",
       " 'val_loss': [0.7693991795474407, 0.7601562662451875, 0.763168547667709],\n",
       " 'val_sparse_categorical_accuracy': [0.7328431346252853,\n",
       "  0.7328431346252853,\n",
       "  0.7328431346252853]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_log.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"I am definitely a republican\"\n",
    "X_sample = np.array(list(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                   | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 21688.00it/s]\u001b[A\n",
      "  0%|                                                                                                   | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 41091.85it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "sample_tweet_gen = BatchGenerator(X_sample,\n",
    "                                  vocab_file=os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt'),\n",
    "                                  seq_len=SEQ_LEN,\n",
    "                                  labels=Y_test,\n",
    "                                  do_lower_case=False,\n",
    "                                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.8169721 , 0.06253494, 0.12049302],\n",
       "       [0.816972  , 0.06253491, 0.12049303],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253491, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049305],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049303],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049305],\n",
       "       [0.816972  , 0.06253494, 0.12049305],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049305],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.8169721 , 0.06253494, 0.12049302],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049305],\n",
       "       [0.816972  , 0.06253494, 0.12049305],\n",
       "       [0.816972  , 0.06253493, 0.12049303],\n",
       "       [0.816972  , 0.06253494, 0.12049303],\n",
       "       [0.816972  , 0.06253493, 0.12049305],\n",
       "       [0.816972  , 0.06253493, 0.12049303]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_predictions = model.predict_generator(sample_tweet_gen, verbose=1)\n",
    "sample_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['congratulations to lorraine miller a trailblazing leader who has served our country with distinction who will do an outstanding job leading the naacp as its interim president and ceo',\n",
       "       'the us supreme court just handed down its decision on the voting rights act vra learn why the vra is the most effective civil rights law in history and why it\\x89ûªs still needed at wwwcivilrightsorgshelby or httpbitlyldfshelby',\n",
       "       'another side effect of obamacare httptcojfkaidbsp', ...,\n",
       "       'what could you do with your k share of the national debt',\n",
       "       'an update on fight to protect religious freedom in military on tony perkins radio show today at  et httptcoiwuosvxg',\n",
       "       'message  obama oppression in cuba will not change while castro brothers r in charge more activists arrested before rally yotambienexijo'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
